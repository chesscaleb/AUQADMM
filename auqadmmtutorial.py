# -*- coding: utf-8 -*-
"""AUQADMMtutorial

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aiWiC4vJcRQrJbz4grcqSj5c4Yzrf7oj
"""

import torch
import torch.nn as nn
import math
import numpy as np
import pywt

def FullLoss(trainset, x):
    loss = nn.MSELoss()
    output = 0
    X, y = trainset
    X = X.view(-1, M)
    input = torch.mm(X, x)
    y = y.reshape(1000,1)
    y = y.type(torch.FloatTensor)
    output += loss(input,y)
    return output
    
def LocalCost(trainset, u, v, lam, w):
    objfunc = FullLoss(trainset, u)
    extra_terms = 0
    diff = v - u
    res = diff + 1/w*lam
    
    extra_terms = (res*w*res).sum()

    return objfunc + extra_terms    

def proximal(rho1, rho2, U, lambdas, diag_Weight_list):
    threshold = torch.nn.Threshold(0,0)
    X = torch.zeros_like(U[0])
    K = rho2*torch.ones_like(diag_Weight_list[0])
    for count, u in enumerate(U):
        X = X + diag_Weight_list[count].clone()*u.clone()-lambdas[count]
        K = K + diag_Weight_list[count]
    value = rho1
    V = 1.0/K*threshold(abs(X)-value)*torch.sign(X)
    return V

import matplotlib.pyplot as plt
import math
import numpy as np
import torch
import torchvision
import torch.optim as optim
from torchvision import transforms, datasets

#Affine Normalization such that all elements 
#of target X are in a range [a,b]
def affine_normalize(a, b, X):
    #a,b: range of cutoff
    #X: target

    q = max(X).item()
    p = min(X).item()
    #print('q,p: ', q, p)
    m = (b-a)/(q-p)
    n = a - m*p
    return m*X+n

#Lanczos
import torch

def manual_Lanczos(f, x, q1, rank):
    ##INPUTS:
    #f: objective function
    #x: the point at which the gradient and Hessian are evaluated
    #q1: initial vector
    #rank: desired rank number for the approximation QTQ^t
    
    ##OUTPUTS:
    #Qt: transpose of Q
    #T: the triadiagonal matrix T
    
    k = 0
    beta = 1.0
    q = torch.zeros_like(q1)
    r = q1
    dim1 = q1.shape[0]
    dim2 = q1.shape[1]
    x.requires_grad = True

    Qt = []
    a = []
    b = []
    
    
    while k < rank:
        q_prev = q.clone().detach()
        q = r/beta
        k = k+1
        
        #Calculate A*q
        y = f(x)
        dydx = torch.autograd.grad(outputs=f(x), inputs=x,
                           create_graph=True, retain_graph=True, only_inputs=True)[0]
        Aq = torch.autograd.grad(outputs=dydx, inputs=x, grad_outputs=q)[0]
        
        alpha = (q * Aq).sum()
        r = Aq - alpha*q - beta*q_prev
        beta = torch.norm(r)
        
        Qt.append(torch.transpose(q,0,1).reshape(dim1*dim2).tolist())
        a.append(alpha.item())
        if k < rank:
            b.append(beta.item())
            
    a = torch.tensor(a); b = torch.tensor(b)
    T = torch.diag(a) + torch.diag(b,1) + torch.diag(b,-1)
    
    return [torch.tensor(Qt).clone().detach(), T.clone().detach()]

#Generate Diagonal Weights, return weight and the normalization factor gamma
def GenerateDiagWeights(trainset, rank, u, a, b):
    dim1 = u.shape[0]
    dim2 = u.shape[1]
    
    def f(c):
        return FullLoss(trainset, c)

    q1 = torch.randn(dim1,dim2)
    q1 = 1.0/torch.norm(q1)*q1
    [Qt, T] = manual_Lanczos(f, u, q1, 10)
    Hessian = torch.mm(torch.transpose(Qt,0,1),torch.mm(T,Qt))

    Hdiag = torch.diagonal(Hessian, 0)
    Hdiag = Hdiag.view(-1)

    Hdiag = affine_normalize(a, b, Hdiag) #Normalize the Hessian such that all elements are in [a, b]
    Hdiag = Hdiag.view(dim1, dim2)

    return Hdiag

#Universal Adaptive Scaling
def universal(a, b, max_iter, current_iter, previous_weight, current_hessian):
    #[a,b]: restriction interval
    #max_iter: maximum number of iterations
    #current_iter: current iteration number, i.e. the ith iteration
    #previous_weight: the previous weight
    #current_hessian: the diagonal of the current Hessian
    
    #Convergence Constant
    C_cg = (b/a-1.0)*max_iter**2+1
    
    h = current_hessian; w = previous_weight
    ck = C_cg/(1+current_iter)**2
    q = w/h
    q = q.view(-1)
    q_max = max(q).item()
    q_min = min(q).item()
    alpha_inf = 1/(1+ck)*q_max
    alpha_sup = (1+ck)*q_min
    alpha = 1.0
    if alpha_inf <= 1 and alpha_sup >= 1:
        alpha = 1.0
    elif alpha_sup <= 1:
        alpha = alpha_sup
    elif alpha_inf >= 1:
        alpha = alpha_inf

    if alpha != 1.0:
        new_weight = affine_normalize(a, b, alpha*h)
    else:
        new_weight = h

    return new_weight

def UpdateLocalLBFGS(U, V, lam, w, trainset, epochs):
    optimizer = optim.LBFGS([U],max_iter=4, history_size=20 ,lr = 1e-3)

    for epoch in range(epochs):
        def closure():
            if torch.is_grad_enabled():
                optimizer.zero_grad()
            loss = LocalCost(trainset, U, V, lam, w)
            if loss.requires_grad:
                loss.backward(retain_graph=True)
            return loss
        optimizer.step(closure)
    return U

def UpdateLambda(U, V, lambdas, w):
    with torch.no_grad():    
        for count, u in enumerate(U):
            lambdas[count] = torch.add(lambdas[count], w[count]*(V - u))
    return lambdas

def UpdateInterval(k, a1, b1, a, b, K):
    if k%K==0:
        i = k*1.0/(K*1.0)
        gam = 1.0/(i+1)**2*b1/a1+1-1/(i+1)**2
        b = gam*a
    return [a,b]

def Generate_and_Classify_Trainsets(Number_of_Samples_each_worker, Target_Dataset_Name, TrainLoader):
  #Number_of_Samples_each_worker: number of samples assigned to each worker, like 2000, 2500 or so
  #Target_Dataset_Name: 'MNIST' or 'CIFAR10'
  #TrainLoader: trainloader containing datasets

  N = Number_of_Samples_each_worker
  if Target_Dataset_Name == 'MNIST':
      M = 28*28
  elif Target_Dataset_Name == 'CIFAR10':
      M = 3072

  Zeros = []; Ones = []; Twos = []; Threes = []; Fours = []; Fives = []; Sixs = []; Sevens = []; Eights = []; Nines = []

  for data in TrainLoader:
      D, L = data
      if L[0] == 0:
          Zeros.append(D)
      elif L[0] == 1:
          Ones.append(D)
      elif L[0] == 2:
          Twos.append(D)
      elif L[0] == 3:
          Threes.append(D)
      elif L[0] == 4:
          Fours.append(D)
      elif L[0] == 5:
          Fives.append(D)
      elif L[0] == 6:
          Sixs.append(D)
      elif L[0] == 7:
          Sevens.append(D)
      elif L[0] == 8:
          Eights.append(D)
      elif L[0] == 9:
          Nines.append(D)

  X0 = Zeros[0].view(-1,M)
  X1 = Ones[0].view(-1,M)
  X2 = Twos[0].view(-1,M)
  X3 = Threes[0].view(-1,M)
  X4 = Fours[0].view(-1,M)
  X5 = Fives[0].view(-1,M)
  X6 = Sixs[0].view(-1,M)
  X7 = Sevens[0].view(-1,M)
  X8 = Eights[0].view(-1,M)
  X9 = Nines[0].view(-1,M)
  L0 = torch.zeros(N, dtype=torch.long)
  L1 = torch.ones(N, dtype=torch.long)
  L2 = 2*torch.ones(N, dtype=torch.long)
  L3 = 3*torch.ones(N, dtype=torch.long)
  L4 = 4*torch.ones(N, dtype=torch.long)
  L5 = 5*torch.ones(N, dtype=torch.long)
  L6 = 6*torch.ones(N, dtype=torch.long)
  L7 = 7*torch.ones(N, dtype=torch.long)
  L8 = 8*torch.ones(N, dtype=torch.long)
  L9 = 9*torch.ones(N, dtype=torch.long)

  for i in range(N-1):
      A = Zeros[i+1].view(-1,M)
      X0 = torch.cat((X0,A),0)
      A = Ones[i+1].view(-1,M)
      X1 = torch.cat((X1,A),0)
      A = Twos[i+1].view(-1,M)
      X2 = torch.cat((X2,A),0)
      A = Threes[i+1].view(-1,M)
      X3 = torch.cat((X3,A),0)
      A = Fours[i+1].view(-1,M)
      X4 = torch.cat((X4,A),0)
      A = Fives[i+1].view(-1,M)
      X5 = torch.cat((X5,A),0)
      A = Sixs[i+1].view(-1,M)
      X6 = torch.cat((X6,A),0)
      A = Sevens[i+1].view(-1,M)
      X7 = torch.cat((X7,A),0)
      A = Eights[i+1].view(-1,M)
      X8 = torch.cat((X8,A),0)
      A = Nines[i+1].view(-1,M)
      X9 = torch.cat((X9,A),0)

  trainset1 = [X0,L0]
  trainset2 = [X1,L1]
  trainset3 = [X2,L2]
  trainset4 = [X3,L3]
  trainset5 = [X4,L4]
  trainset6 = [X5,L5]
  trainset7 = [X6,L6]
  trainset8 = [X7,L7]
  trainset9 = [X8,L8]
  trainset10 = [X9,L9]

  return [trainset1, trainset2, trainset3, trainset4, trainset5, trainset6, trainset7, trainset8, trainset9, trainset10]

# Commented out IPython magic to ensure Python compatibility.
import copy
import torch
import numpy as np
import matplotlib.pyplot as plt
import time

def g(z):
    return 0.5*torch.norm(z, p=1) + 0.005*((z*z).sum())

#LOSS FUNCTION
class AUQADMM:
    def __init__(self, params, regularizer, rho1, rho2, trainsets):
        self.U = params
        dim1 = params[0].shape[0]; dim2 = params[0].shape[1];
        self.dim = 1.0*dim1*dim2
        self.Workers = len(self.U)
        self.regularizer = regularizer
        self.rho1 = rho1
        self.rho2 = rho2
        self.trainsets = trainsets

        self.iteration_number = []
        self.objfunc = []
        self.lambdas = []
        
        self.dlambdas = []
        self.dus = []

        self.Hessians = []
        self.diagonalweights = []

        self.V = torch.zeros_like(self.U[0])

        self.loss = []

        for u in self.U:
            self.lambdas.append(torch.zeros_like(u))
            self.dus.append(torch.zeros_like(u))
            self.dlambdas.append(torch.zeros_like(u))
            self.Hessians.append(0)
            self.diagonalweights.append(0)
    
    def fit(self, wopt, maxiter=1, X_EPOCHS=3, abs_tol=1e-5, rel_tol=1e-4, rank=5, a=0.5, b=1.5, closure=None, Nesterov=False, K=1):
        a0 = a; b0 = b #restriction interval initialization
        primal_residual = [] #primal_residual
        dual_residual = [] #dual_residual
        primal_residual_wopt = [] #primal_residual in wopt norm
        dual_residual_wopt = [] #dual_residual in wopt norm
        
        if Nesterov == True:
            lam_tilda = []
            v_tilda = self.V.clone().detach()
        
        for i in range(maxiter):
            t = time.time()
            
            if i == 0 and Nesterov == True:
                theta = 1.0

            V_previous = self.V.clone().detach()
            
            if Nesterov == True:
                lambda_previous = []

                for it, lam in enumerate(self.lambdas):
                    lambda_previous.append(lam.clone().detach())
                    if i == 0:
                        lam_tilda.append(lam.clone().detach())
                

            #Generate weights and update u
            for it, trainset in enumerate(self.trainsets):
                #Generate Diagonal Hessian Weights
                self.diagonalweights[it] = GenerateDiagWeights(trainset, rank, self.U[it].clone().detach(), a, b)

                #Update u
                if Nesterov == True:
                    self.U[it] = UpdateLocalLBFGS(self.U[it], v_tilda, lam_tilda[it], self.diagonalweights[it], trainset, X_EPOCHS)
                else:
                    self.U[it] = UpdateLocalLBFGS(self.U[it], self.V, self.lambdas[it], self.diagonalweights[it], trainset, X_EPOCHS)
             
            
            #Update Interval
            a, b = UpdateInterval(i+1, a0, b0, a, b, K)
            

            #Save the norm of u and total loss in each iteration
            u_norm = []
            for u in self.U:
                u_norm.append(torch.norm(u))

            #Update v and lambda
            
            with torch.no_grad():
                if Nesterov == True:
                    theta_pre = theta
                    theta = 0.5*(1+np.sqrt(1+4*theta*theta))
                    beta = (theta_pre - 1)/theta
                    
                    self.V = proximal(self.rho1, self.rho2, self.U, lam_tilda, self.diagonalweights)
                    self.lambdas = UpdateLambda(self.U, self.V, lam_tilda, self.diagonalweights)
                    
                    #Update v_tilda and lam_tilda
                    v_tilda = self.V + beta*(self.V - V_previous)
                    for it, lam in enumerate(self.lambdas):
                        lam_tilda[it] = lam + beta*(lam - lambda_previous[it])
                
                else:
                    self.V = proximal(self.rho1, self.rho2, self.U, self.lambdas, self.diagonalweights)
                    self.lambdas = UpdateLambda(self.U, self.V, self.lambdas, self.diagonalweights)
            
            #Save the norm of lambda
            lam_norm = []
            for lam in self.lambdas:
                lam_norm.append(torch.norm(lam))
            
            loss = 0
            for it, trainset in enumerate(self.trainsets):
                loss += FullLoss(trainset, self.V)
            print('f + g %d', loss, end='       ')    
            loss += self.regularizer(self.V)
            self.loss.append(loss)

            #Only for printing and formatting purpose
            info_list = [(i+1), loss]
            C_cg = (b/a-1.0)*maxiter**2+1
            if i == 0:
                print()
                print('Convergence Constant: ', C_cg)
                print()
                print('%s    %s'
#                 % ('k^th',  'f+g'))
            for index in range(len(info_list)):
                if index == 0:
                    print('%d'%info_list[index], end='       ')
                else:
                    print('%.2e'%info_list[index], end='       ')
            print()

            #Save previous weights
            previous_weights = copy.deepcopy(self.diagonalweights)

            with torch.no_grad():
                #Stopping Criteria
                pr = 0
                dr = 0
                pr_wopt = 0
                dr_wopt = 0
                u_norm_sq = 0
                v_norm_sq = (self.V*self.V).sum().item()
                lam_norm_sq = 0
                
                for count, u in enumerate(self.U):
                    pr += ((self.V-u)*(self.V-u)).sum().item()
                    pr_wopt += ((self.V-u)*wopt[count]*(self.V-u)).sum().item()
                    vec = V_previous - self.V
                    dr += (vec*vec).sum().item()
                    dr_wopt += (vec*wopt[count]*vec).sum().item()
                    u_norm_sq += (u*u).sum().item()
                    lam_norm_sq += (self.lambdas[count]*self.lambdas[count]).sum().item()

                pr = np.sqrt(pr)
                pr_wopt = np.sqrt(pr_wopt)
                dr = np.sqrt(dr)
                dr_wopt = np.sqrt(dr_wopt)
                e_pr = np.sqrt(self.dim)*abs_tol+rel_tol*max(np.sqrt(u_norm_sq), np.sqrt(self.Workers*v_norm_sq))
                e_dr = np.sqrt(self.dim)*abs_tol+rel_tol*np.sqrt(lam_norm_sq)
                
                print('primal residual: ', pr)
                print('dual residual:', dr)

            primal_residual.append(pr)
            primal_residual_wopt.append(pr_wopt)
            dual_residual.append(dr)
            dual_residual_wopt.append(dr_wopt)
            time_elapsed = time.time()-t
            print('time used: ', time_elapsed)
            print()

            if pr <= e_pr and dr <= e_dr:
                break
                
        return [self.V, self.loss, i, primal_residual, dual_residual, self.diagonalweights, primal_residual_wopt, dual_residual_wopt, self.U]

from google.colab import drive
drive.mount('/content/drive')
import os
os.chdir("/content/drive/MyDrive/")

#Initial x's, M=28*28 here
M = 28*28
##Upload TrainLoaders.pth, then load it
TrainLoaders = torch.load('TrainLoaders.pth')

##Load target trainloader

#if MNIST:
trainloader_min = TrainLoaders['MNIST'][0]
#if CIFAR10:
trainloader_cifar10 = TrainLoaders['CIFAR'][0]

trainloader_svhn = torch.load('trainloader_svhn.pth')


#Use the function above to generate trainsets
#if MNIST:
trainsets = Generate_and_Classify_Trainsets(1000, 'MNIST', trainloader_min)

trainset1 = trainsets[0]
trainset2 = trainsets[1]
trainset3 = trainsets[2]
trainset4 = trainsets[3]
trainset5 = trainsets[4]
trainset6 = trainsets[5]
trainset7 = trainsets[6]
trainset8 = trainsets[7]
trainset9 = trainsets[8]
trainset10 = trainsets[9]


# 0 and 1 are the coefficients of regularizers, rho1=0 and rho2=1. rho1 is the coeff of |x| and rho2 is the coeff of 0.5*||x||^2.

#Multinomial
M = 28*28; #for MNIST
#M = 3072; #for CIFAR10 or SVHN
x1 = torch.randn(M,1,requires_grad=True)
x2 = torch.randn(M,1,requires_grad=True)
x3 = torch.randn(M,1,requires_grad=True)
x4 = torch.randn(M,1,requires_grad=True)
x5 = torch.randn(M,1,requires_grad=True)
x6 = torch.randn(M,1,requires_grad=True)
x7 = torch.randn(M,1,requires_grad=True)
x8 = torch.randn(M,1,requires_grad=True)
x9 = torch.randn(M,1,requires_grad=True)
x10 = torch.randn(M,1,requires_grad=True)

auqadmm = AUQADMM([x1, x2, x3, x4, x5, x6, x7, x8, x9, x10], g , 0.005, 0.01, [trainset1, trainset2, trainset3, trainset4, trainset5, trainset6, trainset7, trainset8, trainset9, trainset10])

wopt = torch.ones(M, 1)

a = 0.1; b = 1.0 #Restriction interval
result_k_mnist = auqadmm.fit(maxiter=250, X_EPOCHS=30, abs_tol=1e-4, rel_tol=1e-5, rank=5, a=a, b=b, wopt=wopt, Nesterov=False, K=1)

torch.save(result_k_mnist, 'kmnist2.pth')